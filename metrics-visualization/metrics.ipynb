{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import json \n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, get_token_usage, query_inferences\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_inferences_by_day(inferences): \n",
    "    def millis_to_datetime(millis):\n",
    "        return datetime.fromtimestamp(millis / 1000.0)\n",
    "\n",
    "    bucketed_objects = {}\n",
    "    for inf in inferences:\n",
    "        date = millis_to_datetime(inf[\"created_at\"]).date()\n",
    "        if date not in bucketed_objects:\n",
    "            bucketed_objects[date] = []\n",
    "        bucketed_objects[date].append(inf)\n",
    "\n",
    "    return bucketed_objects\n",
    "\n",
    "def get_prompt_and_response_inf_per_day(task_inferences_map_by_day):\n",
    "\n",
    "    prompt_inf_day_map = {}\n",
    "    response_inf_day_map = {}\n",
    "\n",
    "    for day in task_inferences_map_by_day: \n",
    "\n",
    "        for inf in task_inferences_map_by_day.get(day): \n",
    "            prompt_inf_curr_day = prompt_inf_day_map.get(day)\n",
    "\n",
    "            if prompt_inf_curr_day is None: \n",
    "                prompt_inf_curr_day = []\n",
    "\n",
    "            response_inf_curr_day = response_inf_day_map.get(day)\n",
    "            if response_inf_curr_day is None: \n",
    "                response_inf_curr_day = []\n",
    "\n",
    "            prompt_inf = inf[\"inference_prompt\"]\n",
    "            response_inf = inf[\"inference_response\"]\n",
    "\n",
    "            if prompt_inf: \n",
    "                prompt_inf_curr_day.append(prompt_inf)\n",
    "            if response_inf: \n",
    "                response_inf_curr_day.append(response_inf)\n",
    "\n",
    "            prompt_inf_day_map[day] = prompt_inf_curr_day\n",
    "            response_inf_day_map[day] = response_inf_curr_day    \n",
    "\n",
    "    return prompt_inf_day_map, response_inf_day_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the following: \n",
    "\n",
    "- Start Time \n",
    "- End Time \n",
    "- Task Ids \n",
    "- Rule Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now() + timedelta(days=1)\n",
    "start = end - timedelta(days=7)\n",
    "\n",
    "task_ids = [\"<TASK1>\", \"<TASK2>\"]\n",
    "rule_types = [\"ModelHallucinationRuleV2\", \"ModelSensitiveDataRule\", \"PIIDataRule\", \"ToxicityRule\", \"RegexRule\", \"KeywordRule\", \"PromptInjectionRule\"]\n",
    "\n",
    "inferences = query_inferences(start=start, end=end, task_ids=task_ids)\n",
    "\n",
    "print(inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def custom_sort(item):\n",
    "    task_id = item.get('task_id')\n",
    "    if task_id is None:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return task_id\n",
    "\n",
    "sorted_inferences = sorted(inferences, key=custom_sort)\n",
    "\n",
    "tasks_inferences_map = {task_id: list(objects) for task_id, objects in groupby(\n",
    "    sorted_inferences, key=custom_sort)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks_inferences_map: \n",
    "    print(task)\n",
    "    inferences = tasks_inferences_map.get(task)\n",
    "    print(f\"Total num of inferences {len(inferences)}\")\n",
    "    inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "    prompt_inf_day_map, response_inf_day_map = get_prompt_and_response_inf_per_day(inferences_by_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks_inferences_map: \n",
    "    print(task)\n",
    "    inferences = tasks_inferences_map.get(task)\n",
    "    print(f\"Total num of inferences {len(inferences)}\")\n",
    "    inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "    prompt_inf_day_map, response_inf_day_map = get_prompt_and_response_inf_per_day(inferences_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage information \n",
    "usage = get_token_usage(start=start, end=end, groupby_rule_type=True, groupby_task=True)\n",
    "\n",
    "usage_per_task_map = {}\n",
    "\n",
    "for usage_metric in usage: \n",
    "\n",
    "    task_id = usage_metric[\"task_id\"]\n",
    "    rule_type = usage_metric[\"rule_type\"]\n",
    "\n",
    "    task_usage_data =  usage_per_task_map.get(task_id)\n",
    "\n",
    "    if task_usage_data is None: \n",
    "        task_usage_data = {}\n",
    "    \n",
    "    task_usage_data_per_rule = task_usage_data.get(rule_type)\n",
    "\n",
    "    if task_usage_data_per_rule is None: \n",
    "        task_usage_data_per_rule = usage_metric[\"count\"]\n",
    "    \n",
    "\n",
    "    task_usage_data[rule_type] = task_usage_data_per_rule\n",
    "\n",
    "    usage_per_task_map[task_id] = task_usage_data\n",
    "\n",
    "print(usage_per_task_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(f'''\n",
    "*********************************************************************************************\n",
    "      \n",
    "Usage metrics per task. Start: {start} to End: {end}\n",
    "      \n",
    "*********************************************************************************************\n",
    "''')\n",
    "\n",
    "\n",
    "\n",
    "for task in tasks_inferences_map: \n",
    "    print(\"*********************************************************************************************\")\n",
    "    print(task)\n",
    "    inferences = tasks_inferences_map.get(task)\n",
    "    print(f\"Total num of inferences {len(inferences)}\")\n",
    "    inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "    prompt_inf_day_map, response_inf_day_map = get_prompt_and_response_inf_per_day(inferences_by_day)\n",
    "\n",
    "    sorted_days = sorted(inferences_by_day.keys(), reverse=False)\n",
    "\n",
    "    # Total counts \n",
    "    total_inf_counts = [len(inferences_by_day[day]) for day in sorted_days]\n",
    "    prompt_inf_counts = [len(prompt_inf_day_map[day]) for day in sorted_days]\n",
    "    response_inf_counts =  [len(response_inf_day_map[day]) for day in sorted_days]\n",
    "    sorted_days_str = [str(day) for day in sorted_days]\n",
    "\n",
    "    # Specific counts \n",
    "    response_inf_counts_failed = [sum(1 for result in response_inf_day_map[day] if result['result'] == \"Fail\") for day in sorted_days]\n",
    "    response_inf_counts_pass = [sum(1 for result in response_inf_day_map[day] if result['result'] == \"Pass\") for day in sorted_days]\n",
    "\n",
    "    prompt_inf_counts_failed = [sum(1 for result in prompt_inf_day_map[day] if result['result'] == \"Fail\") for day in sorted_days]\n",
    "    prompt_inf_counts_pass = [sum(1 for result in prompt_inf_day_map[day] if result['result'] == \"Pass\") for day in sorted_days]\n",
    "\n",
    "\n",
    "    # Plotting the prompt data \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts, marker='o', color='black', linestyle='-', label = 'Prompt Inf Count')\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "\n",
    "    plt.ylabel('Inferences')\n",
    "    plt.title(f\"{task} prompt inferences\")\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the response data \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_days_str, response_inf_counts, marker='o', color='black', linestyle='-', label = 'Response Inf Count')\n",
    "    plt.plot(sorted_days_str, response_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "    plt.plot(sorted_days_str, response_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "    plt.ylabel('Inferences')\n",
    "    plt.title(f\"{task} response inferences\")\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Token Usage Data \n",
    "    print(\"Token usage information\")\n",
    "    print('''\n",
    "    User input (raw input by user) - metric produced by Arthur\n",
    "    Prompt (raw input by user + Shield prompt augmentation) - metric produced by Azure\n",
    "    Completion (raw input by user + Shield prompt augmentation) - metric produced by Azure\n",
    "    ''')\n",
    "    print(json.dumps(usage_per_task_map.get(task), indent=4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
