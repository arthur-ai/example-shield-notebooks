{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json \n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, get_token_usage, query_inferences, search_tasks\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Local Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_inferences_by_day(inferences): \n",
    "    def millis_to_datetime(millis):\n",
    "        return datetime.fromtimestamp(millis / 1000.0)\n",
    "\n",
    "    bucketed_objects = {}\n",
    "    for inf in inferences:\n",
    "        date = millis_to_datetime(inf[\"created_at\"]).date()\n",
    "        if date not in bucketed_objects:\n",
    "            bucketed_objects[date] = []\n",
    "        bucketed_objects[date].append(inf)\n",
    "\n",
    "    return bucketed_objects\n",
    "\n",
    "def get_prompt_and_response_inf_per_day(task_inferences_map_by_day):\n",
    "\n",
    "    prompt_inf_day_map = {}\n",
    "    response_inf_day_map = {}\n",
    "\n",
    "    for day in task_inferences_map_by_day: \n",
    "\n",
    "        for inf in task_inferences_map_by_day.get(day): \n",
    "            prompt_inf_curr_day = prompt_inf_day_map.get(day)\n",
    "\n",
    "            if prompt_inf_curr_day is None: \n",
    "                prompt_inf_curr_day = []\n",
    "\n",
    "            response_inf_curr_day = response_inf_day_map.get(day)\n",
    "            if response_inf_curr_day is None: \n",
    "                response_inf_curr_day = []\n",
    "\n",
    "            prompt_inf = inf[\"inference_prompt\"]\n",
    "            response_inf = inf[\"inference_response\"]\n",
    "\n",
    "            if prompt_inf: \n",
    "                prompt_inf_curr_day.append(prompt_inf)\n",
    "            if response_inf: \n",
    "                response_inf_curr_day.append(response_inf)\n",
    "\n",
    "            prompt_inf_day_map[day] = prompt_inf_curr_day\n",
    "            response_inf_day_map[day] = response_inf_curr_day    \n",
    "\n",
    "    return prompt_inf_day_map, response_inf_day_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Retrieve relevant inference and task infromation for a given period of time\n",
    "\n",
    "Enter the following: \n",
    "\n",
    "- Start Time \n",
    "- End Time \n",
    "- Task Ids (or ALL_TASKS = TRUE for all tasks in the environment)\n",
    "- Rule Types \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now() + timedelta(days=1)\n",
    "start = end - timedelta(days=30)\n",
    "\n",
    "task_ids = [\"<TASK1>\", \"<TASK2>\"]\n",
    "\n",
    "ALL_TASKS = True\n",
    "\n",
    "rule_types = [\"ModelHallucinationRuleV2\", \"ModelSensitiveDataRule\", \"PIIDataRule\", \"ToxicityRule\", \"RegexRule\", \"KeywordRule\", \"PromptInjectionRule\"]\n",
    "\n",
    "if ALL_TASKS: \n",
    "    inferences = query_inferences(start=start, end=end)\n",
    "else: \n",
    "    inferences = query_inferences(start=start, end=end, task_ids=task_ids)\n",
    "\n",
    "# Get additional task information, like name, for enrichemnt purposes \n",
    "tasks = search_tasks(all_tasks=ALL_TASKS, task_ids=task_ids)\n",
    "\n",
    "all_tasks_info_map = {task[\"id\"]: task for task in tasks}\n",
    "\n",
    "print(inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort(item):\n",
    "    task_id = item.get('task_id')\n",
    "    if task_id is None:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return task_id\n",
    "\n",
    "sorted_inferences = sorted(inferences, key=custom_sort)\n",
    "\n",
    "tasks_inferences_map = {task_id: list(objects) for task_id, objects in groupby(\n",
    "    sorted_inferences, key=custom_sort)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage information \n",
    "usage = get_token_usage(start=start, end=end, groupby_rule_type=True, groupby_task=True)\n",
    "\n",
    "usage_per_task_map = {}\n",
    "\n",
    "for usage_metric in usage: \n",
    "\n",
    "    task_id = usage_metric[\"task_id\"]\n",
    "    rule_type = usage_metric[\"rule_type\"]\n",
    "\n",
    "    task_usage_data =  usage_per_task_map.get(task_id)\n",
    "\n",
    "    if task_usage_data is None: \n",
    "        task_usage_data = {}\n",
    "    \n",
    "    task_usage_data_per_rule = task_usage_data.get(rule_type)\n",
    "\n",
    "    if task_usage_data_per_rule is None: \n",
    "        task_usage_data_per_rule = usage_metric[\"count\"]\n",
    "    \n",
    "\n",
    "    task_usage_data[rule_type] = task_usage_data_per_rule\n",
    "\n",
    "    usage_per_task_map[task_id] = task_usage_data\n",
    "\n",
    "print(usage_per_task_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Total Inference & Token Usage across all tasks specified for the given time period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "all_inf_propmt_day_map, all_inf_response_day_map = get_prompt_and_response_inf_per_day(all_inferences_by_day)\n",
    "\n",
    "\n",
    "print(f'''\n",
    "*********************************************************************************************\n",
    "      \n",
    "Usage metrics per task. Start: {start} to End: {end}\n",
    "\n",
    "Tasks: {all_tasks_info_map.keys()}\n",
    "      \n",
    "*********************************************************************************************\n",
    "''')\n",
    "\n",
    "sorted_days = sorted(all_inferences_by_day.keys(), reverse=False)\n",
    "min_date = min(sorted_days)\n",
    "max_date = max(sorted_days)\n",
    "all_dates = [min_date + timedelta(days=i) for i in range((max_date - min_date).days + 1)]\n",
    "\n",
    "all_dates_str = [str(day) for day in all_dates]\n",
    "\n",
    "total_inf_counts = []\n",
    "prompt_inf_counts = []\n",
    "response_inf_counts = []\n",
    "response_inf_counts_failed = []\n",
    "response_inf_counts_pass = []\n",
    "prompt_inf_counts_failed = []\n",
    "prompt_inf_counts_pass = []\n",
    "\n",
    "for day in all_dates:\n",
    "    if day in all_inferences_by_day:\n",
    "        total_inf_counts.append(len(all_inferences_by_day[day]))\n",
    "    else:\n",
    "        total_inf_counts.append(0)\n",
    "    \n",
    "    if day in all_inf_propmt_day_map:\n",
    "        prompt_inf_counts.append(len(all_inf_propmt_day_map[day]))\n",
    "        prompt_inf_counts_failed.append(sum(1 for result in all_inf_propmt_day_map[day] if result['result'] == \"Fail\"))\n",
    "        prompt_inf_counts_pass.append(sum(1 for result in all_inf_propmt_day_map[day] if result['result'] == \"Pass\"))\n",
    "    else:\n",
    "        prompt_inf_counts.append(0)\n",
    "        prompt_inf_counts_failed.append(0)\n",
    "        prompt_inf_counts_pass.append(0)\n",
    "    \n",
    "    if day in all_inf_response_day_map:\n",
    "        response_inf_counts.append(len(all_inf_response_day_map[day]))\n",
    "        response_inf_counts_failed.append(sum(1 for result in all_inf_response_day_map[day] if result['result'] == \"Fail\"))\n",
    "        response_inf_counts_pass.append(sum(1 for result in all_inf_response_day_map[day] if result['result'] == \"Pass\"))\n",
    "    else:\n",
    "        response_inf_counts.append(0)\n",
    "        response_inf_counts_failed.append(0)\n",
    "        response_inf_counts_pass.append(0)\n",
    "        \n",
    "# Plotting the prompt data \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(all_dates_str, prompt_inf_counts, marker='o', color='black', linestyle='-', label = 'Prompt Inf Count')\n",
    "plt.plot(all_dates_str, prompt_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "plt.plot(all_dates_str, prompt_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "\n",
    "plt.ylabel('Inferences')\n",
    "plt.title(f\"Prompt inferences\")\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the response data \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(all_dates_str, response_inf_counts, marker='o', color='black', linestyle='-', label = 'Response Inf Count')\n",
    "plt.plot(all_dates_str, response_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "plt.plot(all_dates_str, response_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "plt.ylabel('Inferences')\n",
    "plt.title(f\"Response inferences\")\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Inference & Token Usage information per task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "*********************************************************************************************\n",
    "      \n",
    "Usage metrics per task. Start: {start} to End: {end}\n",
    "      \n",
    "*********************************************************************************************''')\n",
    "\n",
    "sorted_tasks_inferences_map = dict(sorted(tasks_inferences_map.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "\n",
    "\n",
    "for key, value in sorted_tasks_inferences_map.items():\n",
    "    print(f\"Task: {key}, Num inferences: {len(value)}\")\n",
    "\n",
    "\n",
    "for task_id in sorted_tasks_inferences_map: \n",
    "    print(\"*********************************************************************************************\")\n",
    "    task_info = all_tasks_info_map.get(task_id)\n",
    "    if task_info is None: \n",
    "        task_name = task_id\n",
    "    else: \n",
    "        task_name = task_info[\"name\"]\n",
    "    print(f\"Task Name: {task_name}\")\n",
    "    print(f\"Task Id: {task_id}\")\n",
    "\n",
    "    inferences = sorted_tasks_inferences_map.get(task_id)\n",
    "\n",
    "    print(type(inferences))\n",
    "\n",
    "    print(f\"Total num of inferences over selected time period {len(inferences)}\")\n",
    "    inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "    prompt_inf_day_map, response_inf_day_map = get_prompt_and_response_inf_per_day(inferences_by_day)\n",
    "\n",
    "    sorted_days = sorted(inferences_by_day.keys(), reverse=False)\n",
    "\n",
    "    # Total counts \n",
    "    total_inf_counts = [len(inferences_by_day[day]) for day in sorted_days]\n",
    "    prompt_inf_counts = [len(prompt_inf_day_map[day]) for day in sorted_days]\n",
    "    response_inf_counts =  [len(response_inf_day_map[day]) for day in sorted_days]\n",
    "    sorted_days_str = [str(day) for day in sorted_days]\n",
    "\n",
    "    # Specific counts \n",
    "    response_inf_counts_failed = [sum(1 for result in response_inf_day_map[day] if result['result'] == \"Fail\") for day in sorted_days]\n",
    "    response_inf_counts_pass = [sum(1 for result in response_inf_day_map[day] if result['result'] == \"Pass\") for day in sorted_days]\n",
    "\n",
    "    prompt_inf_counts_failed = [sum(1 for result in prompt_inf_day_map[day] if result['result'] == \"Fail\") for day in sorted_days]\n",
    "    prompt_inf_counts_pass = [sum(1 for result in prompt_inf_day_map[day] if result['result'] == \"Pass\") for day in sorted_days]\n",
    "\n",
    "\n",
    "    # Plotting the prompt data \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts, marker='o', color='black', linestyle='-', label = 'Prompt Inf Count')\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "    plt.plot(sorted_days_str, prompt_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "\n",
    "    plt.ylabel('Inferences')\n",
    "    plt.title(f\"\\\"{task_name}\\\" prompt inferences\")\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the response data \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_days_str, response_inf_counts, marker='o', color='black', linestyle='-', label = 'Response Inf Count')\n",
    "    plt.plot(sorted_days_str, response_inf_counts_failed, marker='o', color='red', linestyle='--', label = 'Failures')\n",
    "    plt.plot(sorted_days_str, response_inf_counts_pass, marker='o', color='green', linestyle='--', label = 'Success')\n",
    "    plt.ylabel('Inferences')\n",
    "    plt.title(f\"\\\"{task_name}\\\" response inferences\")\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Token Usage Data \n",
    "    print(\"Token usage information\")\n",
    "    print('''\n",
    "    User input (raw input by user) - metric produced by Arthur\n",
    "    Prompt (raw input by user + Shield prompt augmentation) - metric produced by Azure\n",
    "    Completion (raw input by user + Shield prompt augmentation) - metric produced by Azure\n",
    "    ''')\n",
    "    print(json.dumps(usage_per_task_map.get(task_id), indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Rule status information per task and rule type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "*********************************************************************************************\n",
    "      \n",
    "Rule status details per task (prompt + reponse). Start: {start} to End: {end}\n",
    "      \n",
    "*********************************************************************************************\n",
    "''')\n",
    "for task in tasks_inferences_map: \n",
    "    print(\"*********************************************************************************************\")\n",
    "    inferences = tasks_inferences_map.get(task)\n",
    "    task_info = all_tasks_info_map.get(task)\n",
    "    if task_info is None: \n",
    "        task_name = task_id\n",
    "    else: \n",
    "        task_name = task_info[\"name\"]\n",
    "    print(f\"Task Name: {task_name}\")\n",
    "    print(f\"Task Id: {task}\")\n",
    "    print(f\"Total num of inferences {len(inferences)}\")\n",
    "    inferences_by_day = bucket_inferences_by_day(inferences)\n",
    "\n",
    "    prompt_inf_day_map, response_inf_day_map = get_prompt_and_response_inf_per_day(inferences_by_day)\n",
    "\n",
    "\n",
    "    print(\"******************** Prompt rule statuses per day by rule type ***************************\")\n",
    "    results_by_day_prompt = {}\n",
    "    for day in prompt_inf_day_map: \n",
    "        result_buckets_prompt = {}\n",
    "        for prompt_inf in prompt_inf_day_map[day]:\n",
    "\n",
    "            prompt_rule_results = prompt_inf[\"prompt_rule_results\"]\n",
    "            for result in prompt_rule_results:\n",
    "                rule_type = result[\"rule_type\"]\n",
    "                result = result[\"result\"]\n",
    "                if rule_type not in result_buckets_prompt:\n",
    "                    result_buckets_prompt[rule_type] = {\"Pass\": 0, \"Fail\": 0, \"Skipped\": 0, \"Unavailable\": 0}\n",
    "                result_buckets_prompt[rule_type][result] += 1\n",
    "        results_by_day_prompt[str(day)] = result_buckets_prompt\n",
    "    print(json.dumps(results_by_day_prompt, indent=4))\n",
    "\n",
    "    dates = list(results_by_day_prompt.keys())\n",
    "    sorted_days = sorted(results_by_day_prompt.keys(), reverse=False)\n",
    "    \n",
    "    rules = set(rule for date in dates for rule in results_by_day_prompt[date].keys())\n",
    "    statuses = ['Pass', 'Fail', 'Skipped', 'Unavailable']\n",
    "    colors = {'Pass': 'green', 'Fail': 'red', 'Skipped': 'grey', 'Unavailable': 'black'}\n",
    "\n",
    "    for rule in rules:\n",
    "        pass_counts = [results_by_day_prompt[date].get(rule, {}).get('Pass', 0) for date in sorted_days]\n",
    "        fail_counts = [results_by_day_prompt[date].get(rule, {}).get('Fail', 0) for date in sorted_days]\n",
    "        skip_counts = [results_by_day_prompt[date].get(rule, {}).get('Skipped', 0) for date in sorted_days]\n",
    "        unavailable_counts = [results_by_day_prompt[date].get(rule, {}).get('Unavailable', 0) for date in sorted_days]\n",
    "\n",
    "        pass_total = sum(pass_counts)\n",
    "        fail_total = sum(fail_counts)\n",
    "        skip_total = sum(skip_counts)\n",
    "        unavailable_total = sum(unavailable_counts)\n",
    "\n",
    "        bar_width = 0.2\n",
    "        index = range(len(sorted_days))\n",
    "\n",
    "        plt.bar(index, pass_counts, bar_width, label='Pass', color=colors[\"Pass\"])\n",
    "        plt.bar([i + bar_width for i in index], fail_counts, bar_width, label='Fail', color=colors[\"Fail\"])\n",
    "        plt.bar([i + 2 * bar_width for i in index], skip_counts, bar_width, label='Skipped', color=colors[\"Skipped\"])\n",
    "        plt.bar([i + 3 * bar_width for i in index], unavailable_counts, bar_width, label='Unavailable', color=colors[\"Unavailable\"])\n",
    "\n",
    "        text_strings = [\n",
    "            f'Totals\\nPass: {pass_total} Fail: {fail_total}',\n",
    "            f'Skipped: {skip_total} Unavailable: {unavailable_total}'\n",
    "        ]\n",
    "        \n",
    "        text = '\\n'.join(text_strings)\n",
    "        fig = plt.gcf()\n",
    "        fig.text(0.05, 0.95, text, ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'{rule} Statuses (Prompt)')\n",
    "        plt.xticks([i + 1.5 * bar_width for i in index], sorted_days, rotation=45, ha='right') \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"******************** Response rule statuses per day by rule type ***************************\")\n",
    "    results_by_day = {}\n",
    "    for day in response_inf_day_map: \n",
    "        result_buckets = {}\n",
    "        for response_inf in response_inf_day_map[day]:\n",
    "            response_rule_results = response_inf[\"response_rule_results\"]\n",
    "            for result in response_rule_results:\n",
    "                rule_type = result[\"rule_type\"]\n",
    "                result = result[\"result\"]\n",
    "                if rule_type not in result_buckets:\n",
    "                    result_buckets[rule_type] = {\"Pass\": 0, \"Fail\": 0, \"Skip\": 0, \"Unavailable\": 0}\n",
    "                result_buckets[rule_type][result] += 1\n",
    "        results_by_day[str(day)] = result_buckets\n",
    "    print(json.dumps(results_by_day, indent=4))\n",
    "\n",
    "    dates = list(results_by_day.keys())\n",
    "    sorted_days = sorted(results_by_day.keys(), reverse=False)\n",
    "    \n",
    "    rules = set(rule for date in dates for rule in results_by_day[date].keys())\n",
    "    statuses = ['Pass', 'Fail', 'Skipped', 'Unavailable']\n",
    "    colors = {'Pass': 'green', 'Fail': 'red', 'Skipped': 'grey', 'Unavailable': 'black'}\n",
    "\n",
    "    for rule in rules:\n",
    "        pass_counts = [results_by_day[date].get(rule, {}).get('Pass', 0) for date in sorted_days]\n",
    "        fail_counts = [results_by_day[date].get(rule, {}).get('Fail', 0) for date in sorted_days]\n",
    "        skip_counts = [results_by_day[date].get(rule, {}).get('Skipped', 0) for date in sorted_days]\n",
    "        unavailable_counts = [results_by_day[date].get(rule, {}).get('Unavailable', 0) for date in sorted_days]\n",
    "\n",
    "        pass_total = sum(pass_counts)\n",
    "        fail_total = sum(fail_counts)\n",
    "        skip_total = sum(skip_counts)\n",
    "        unavailable_total = sum(unavailable_counts)\n",
    "\n",
    "        bar_width = 0.1\n",
    "        index = range(len(sorted_days))\n",
    "\n",
    "        plt.bar(index, pass_counts, bar_width, label='Pass', color=colors[\"Pass\"])\n",
    "        plt.bar([i + bar_width for i in index], fail_counts, bar_width, label='Fail', color=colors[\"Fail\"])\n",
    "        plt.bar([i + 2 * bar_width for i in index], skip_counts, bar_width, label='Skipped', color=colors[\"Skipped\"])\n",
    "        plt.bar([i + 3 * bar_width for i in index], unavailable_counts, bar_width, label='Unavailable', color=colors[\"Unavailable\"])\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'{rule} Statuses (Response)')\n",
    "        plt.xticks([i + 1.5 * bar_width for i in index], sorted_days, rotation=45, ha='right') \n",
    "        plt.legend()\n",
    "\n",
    "        text_strings = [\n",
    "            f'Totals\\nPass: {pass_total} Fail: {fail_total}',\n",
    "            f'Skipped: {skip_total} Unavailable: {unavailable_total}'\n",
    "        ]\n",
    "        \n",
    "        text = '\\n'.join(text_strings)\n",
    "        fig = plt.gcf()\n",
    "        fig.text(0.05, 0.95, text, ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALL_TASKS: \n",
    "    unavailables = query_inferences(start, end, rule_types=[\"ModelHallucinationRuleV2\"], rule_statuses=[\"Unavailable\"])\n",
    "    skipped = query_inferences(start, end, rule_types=[\"ModelHallucinationRuleV2\"], rule_statuses=[\"Skipped\"])\n",
    "\n",
    "else: \n",
    "    unavailables = query_inferences(start, end, task_ids=task_ids, rule_types=[\"ModelHallucinationRuleV2\"], rule_statuses=[\"Unavailable\"])\n",
    "    skipped = query_inferences(start, end, task_ids=task_ids, rule_types=[\"ModelHallucinationRuleV2\"], rule_statuses=[\"Skipped\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
