{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Rule Demonstration Notebook (V2)\n",
    "\n",
    "In this notebook we will walk through the following steps in order to demonstrate what the Shield Hallucination Rule V2 is capable of.\n",
    "\n",
    "**Check Outputs:**\n",
    "- Hallucination detected.\n",
    "- No hallucination detected. \n",
    "- Not evaluated for hallucinaion.\n",
    "\n",
    "**Prerequisites:** \n",
    "- Shield environment URL\n",
    "- API key\n",
    "\n",
    "**Steps**\n",
    "1. Create a new task for Hallucination evaluation \n",
    "2. Arthur Benchmark dataset evaluation \n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "   2. View our results \n",
    "3. Load additional dataset (non benchmark) to demonstrate the 'Not evaluated for hallucination' label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Shield Test Env Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "import sys\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, set_up_task_and_rule, task_prompt_validation, task_response_validation, archive task\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.Setup: Configure a test task and enable Hallucination Rule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_rule_config =  {\n",
    "    \"name\": \"Hallucination Rule\",\n",
    "    \"type\": \"ModelHallucinationRuleV2\",\n",
    "    \"apply_to_prompt\": False,\n",
    "    \"apply_to_response\": True\n",
    "}\n",
    "\n",
    "# Create task, archive all rules except the one we pass, create the rule we pass \n",
    "hallucination_rule, hallucination_task = set_up_task_and_rule(hallucination_rule_config, \"hallucination-task-example-notebook\")\n",
    "\n",
    "print(hallucination_rule)\n",
    "print(hallucination_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Arthur benchmark dataset evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "### Label for each claim\n",
    "\n",
    "Hallucination detection in Shield involves labeling each claim within an LLM response as valid or invalid. Behind the scenes, we use our own custom parser to convert text into claims, such that one claim typically corresponds to one sentence or list item within the LLM response.\n",
    "\n",
    "The label `False` for a claim indicates that the claim is not supported by the context. It does NOT mean the claim is false on its own - true claims are considered hallucinations if they lack evidence in the provided context.\n",
    "\n",
    "### Datasets chosen\n",
    "The two datasets we use here to benchmark the hallucination check are based on public datasets available on HuggingFace.\n",
    "\n",
    "Each row contains a context, an llm_response, and a binary_label for each claim in the LLM response (the binary_label for each row is a string of one or more labels, one label for each claim). All other columns can be ignored.\n",
    "\n",
    "The first is based on the [Dolly dataset by Databricks](https://huggingface.co/datasets/databricks/databricks-dolly-15k) - this dataset was compiled with LLM responses based on user-provided contexts made by Databricks employees in early 2023. We have extended the dataset with examples of hallucinations (synthetically generated by gpt-3.5-turbo).\n",
    "\n",
    "The second is based on the [WikiBio GPT-3 Hallucination dataset by the University of Cambridge](https://huggingface.co/datasets/potsawee/wiki_bio_gpt3_hallucination) - this dataset was compiled by prompting GPT-3 to write biographies, many of which contain hallucinated facts.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First, we load the two benchmark datasets, each of which has 1 or more labels per response (for each 'claim' within the response).\n",
    "\n",
    "The hallucination check returns a label for each claim. \n",
    "\n",
    "We format the labels here to be consistent across datasets: comma-separated with no list markers like `[` or `]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = ['dolly', 'wikibio']\n",
    "dfs = [pd.read_csv(f\"./arthur_benchmark_datasets/hallucination_{name}.csv\") for name in df_names]\n",
    "for df in dfs:\n",
    "    for c in ['label', 'binary_label']:\n",
    "        if c in df.columns:\n",
    "            for i, row in df.iterrows():\n",
    "                df.loc[i,c] = df.loc[i,c].replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[1].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's validate that the task was created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(hallucination_task[\"rules\"]) > 1):\n",
    "    raise Exception(\"Cannot have more than one rule enabled for this test.\")\n",
    "else: \n",
    "    if hallucination_task[\"rules\"][0][\"type\"] != \"ModelHallucinationRuleV2\":\n",
    "            raise Exception(\"Invalid rule type enabled. Must be ModelHallucinationRuleV2.\")\n",
    "    else: \n",
    "         print(f\"Valid task {hallucination_task}\")\n",
    "task_id = hallucination_task[\"id\"]\n",
    "rule_id = hallucination_rule[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define two evaluation functions:\n",
    "\n",
    "`shield_hallucination_evaluation` returns a comma-separated string of labels that Shield is returning for each claim in the response.\n",
    "\n",
    "`daily_shield_hallucination_benchmark` runs `shield_hallucination_evaluation` and saves the results to a new dataframe stamped with today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shield_hallucination_evaluation(row, task_id, rule_id, verbose=True): \n",
    "    \"\"\"Returns the result of a shield hallucination rule as a string\n",
    "\n",
    "    Args:\n",
    "        row: row from pandas dataframe with columns `llm_response` and `context`\n",
    "        task_id: str, UUID for the hallucination task created above\n",
    "        rule_id: str, UUID for the hallucination rule created above\n",
    "    Returns:\n",
    "        result_string: str, comma-separated string of labels that shield is returning for each claim in the response\n",
    "    \"\"\"\n",
    "    shield_prompt_inference = task_prompt_validation(\"dummy\", 1, task_id)\n",
    "    inference_id = shield_prompt_inference[\"inference_id\"]\n",
    "    if verbose: print(\"===============\\n\", row)\n",
    "    shield_result = task_response_validation(row.llm_response, row.context, inference_id, task_id)\n",
    "    for rule_result in shield_result[\"rule_results\"]:\n",
    "        if rule_result[\"id\"] == rule_id:\n",
    "            try: \n",
    "                result_string = str([x['valid'] for x in rule_result['details']['claims']]).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                result_explanations = str([x['reason'] for x in rule_result['details']['claims']]).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            except Exception as e: \n",
    "                result_string = \"Error when running.\"\n",
    "                result_explanations = \"Error when running.\"\n",
    "            if verbose: print(result_string)\n",
    "            return result_string, result_explanations \n",
    "\n",
    "def daily_shield_hallucination_benchmark(dfs, df_names, task_id, rule_id, verbose=True, n_test=None):\n",
    "    \"\"\"Run the benchmark datasets and save them with shield hallucination results as new csvs marked with today's date\n",
    "    \n",
    "    Args:\n",
    "        task_id: str, UUID for the hallucination task created above\n",
    "        rule_id: str, UUID for the hallucination rule created above\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for df, name in zip(dfs, df_names):\n",
    "        if n_test: \n",
    "            df = df.head(n_test)\n",
    "        print(\"Benchmarking on dataset\", name)\n",
    "        shield_results = [shield_hallucination_evaluation(row, task_id, rule_id, verbose=verbose) for _, row in df.iterrows()]\n",
    "        df[\"shield_result\"] = [x[0] for x in shield_results]\n",
    "        df[\"shield_reason\"] = [x[1] for x in shield_results]\n",
    "        df.to_csv(f\"./results/hallucination_v2_benchmark_{name}_{current_date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the benchmark function which will save our results to analyze below\n",
    "\n",
    "**REMOVE THE FIRST LINE TO RUN ENTIRE DATASET EVALUATION - IT WILL TAKE TIME AND CONSUME AOAI CREDITS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_shield_hallucination_benchmark(dfs, df_names, task_id, rule_id, n_test=5)\n",
    "# daily_shield_hallucination_benchmark(dfs, df_names, task_id, rule_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we get multiple labels per response (for each claim in the response), we can measure performance both at response level and at the claim level.\n",
    "\n",
    "Response-level performance means just looking at the Shield result - did the Shield rule fail or not? Was there a hallucination *somewhere* in the LLM response? Perfect response-level performance does not require labeling individual claims within a response accurately - it only requires for the Shield result to fail in correspondence with the presence of hallucination anywhere in an LLM response.\n",
    "\n",
    "Claim-level performance is more strict - which claims did the shield result label as valid or invalid? Are those the claims that are actually faithful to the context?\n",
    "\n",
    "You can typically expect response-level performance to be stronger than claim-level performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "def response_level_performance(df):\n",
    "    \"\"\"Evaluates the performance of shield hallucination labels at the response level\"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "    for i, row in df.iterrows():\n",
    "        if row.binary_label==False or 'false' in row.binary_label.lower():\n",
    "            y_true.append(True)\n",
    "        else:\n",
    "            y_true.append(False)\n",
    "\n",
    "        if row.shield_result==False or 'false' in row.shield_result.lower():\n",
    "            y_pred.append(True)\n",
    "        else:\n",
    "            y_pred.append(False)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def claim_level_performance(df):\n",
    "    \"\"\"Evaluates the performance of shield hallucination labels at the claim level\"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "    for i, row in df.iterrows():\n",
    "        if row.shield_result != \"Error when running.\": \n",
    "            labels = row.binary_label.split(\",\")\n",
    "            preds = row.shield_result.split(\",\")\n",
    "            if len(labels) == len(preds):\n",
    "                y_true.extend([x.strip().lower()=='false' for x in labels])\n",
    "                y_pred.extend([x.strip().lower()=='false' for x in preds])\n",
    "            else:\n",
    "                print(\"no including index\", i, \"(inconsistent # of labels)\")\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "for name in df_names:\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~\\n\\n%%%\", name, \"%%%\\n\")\n",
    "    result_df = pd.read_csv(f\"results/hallucination_v2_benchmark_{name}_{current_date}.csv\")\n",
    "    p, r, f = response_level_performance(result_df)\n",
    "    print(f\"\\n>>>>Response level hallucination detection performance:\\n\\nPrecision: {p}\\nRecall: {r}\\nF1: {f}\")\n",
    "    p, r, f = claim_level_performance(result_df)\n",
    "    print(f\"\\n>>>>Claim level hallucination detection performance:\\n\\nPrecision: {p}\\nRecall: {r}\\nF1: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Load additional datasets to demonstrate the \"Not evaluated for hallucination\" label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_df_names = ['python_dialogue']\n",
    "more_dfs = [pd.read_csv(f\"./datasets/hallucination_{name}.csv\") for name in more_df_names]\n",
    "for df in more_dfs:\n",
    "    for c in ['label', 'binary_label']:\n",
    "        if c in df.columns:\n",
    "            for i, row in df.iterrows():\n",
    "                df.loc[i,c] = df.loc[i,c].replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE THE FIRST LINE TO RUN ENTIRE DATASET EVALUATION - IT WILL TAKE TIME AND CONSUME AOAI CREDITS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_shield_hallucination_benchmark(more_dfs, more_df_names, task_id, rule_id, n_test=5)\n",
    "# daily_shield_hallucination_benchmark(more_dfs, more_df_names, task_id, rule_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(f\"results/hallucination_v2_benchmark_python_dialogue_{current_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Delete Test Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_task(hallucination_task[\"id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
