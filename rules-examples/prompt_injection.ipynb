{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Injection Rule Demonstration Notebook\n",
    "\n",
    "In this notebook we will walk through the following steps in order to demonstrate what the Shield Prompt Injection Rule is capable of.\n",
    "\n",
    "- Role Play \n",
    "- Instruction Manipulation  \n",
    "\n",
    "Pre  Requisites: \n",
    "- A Shield env and API key.\n",
    "\n",
    "1. Create a new task for Prompt Injection evaluation \n",
    "2. Arthur Benchmark dataset evaluation \n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "   2. View our results \n",
    "3. Additional examples evaluation using datasets referenced in our documentation: https://shield.docs.arthur.ai/docs/prompt-injection#benchmarks\n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "    2. View our results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Shield Test Env Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, set_up_task_and_rule, run_shield_evaluation\n",
    "from analysis_utils import print_performance_metrics, granular_result_dfs\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.Setup: Configure a test task and enable Prompt Injection Rule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_injection_rule_config =  {\n",
    "    \"name\": \"Prompt Injection Rule\",\n",
    "    \"type\": \"PromptInjectionRule\",\n",
    "    \"apply_to_prompt\": True,\n",
    "    \"apply_to_response\": False\n",
    "}\n",
    "\n",
    "# Create task, archive all rules except the one we pass, create the rule we pass \n",
    "prompt_injection_rule, prompt_injection_task = set_up_task_and_rule(prompt_injection_rule_config, \"prompt-injection-task\")\n",
    "\n",
    "print(prompt_injection_rule)\n",
    "print(prompt_injection_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Arthur benchmark dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_injection_benchmark_df_arthur = pd.read_csv(\"./arthur_benchmark_datasets/prompt_injection_benchmark.csv\")\n",
    "\n",
    "prompt_injection_benchmark_df_arthur[\"label\"] = prompt_injection_benchmark_df_arthur[\"labels\"].map({0: False, 1: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_injection_benchmark_df_arthur = run_shield_evaluation(prompt_injection_benchmark_df_arthur, prompt_injection_task, prompt_injection_rule)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "prompt_injection_benchmark_df_arthur.to_csv(f\"./results/prompt_injection_benchmark_df_arthur{current_datetime}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(prompt_injection_benchmark_df_arthur)\n",
    "\n",
    "arthur_fn, arthur_fp, arthur_tp, arthur_tn = granular_result_dfs(prompt_injection_benchmark_df_arthur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arthur_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Load benchmark datasets from https://shield.docs.arthur.ai/docs/prompt-injection#benchmarks\n",
    "\n",
    "- **DISCLAIMER**: This is for demonstration and guidance purposes only and does not reflect the performance of the model behind the Shield score, as sampling techniques may not be optimal.\n",
    "- **DISCLAIMER 2**: This dataset contains German and Arthur prompt injection is only trained on English. We do our best to filter it out, but some German may slip through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "number_samples_from_each = 50\n",
    "\n",
    "prompt_injections_dataset = load_dataset(\"deepset/prompt-injections\")\n",
    "\n",
    "prompt_injections_dataset_combined = concatenate_datasets([prompt_injections_dataset['train'], prompt_injections_dataset['test']])\n",
    "\n",
    "prompt_injections_dataset_combined_df = prompt_injections_dataset_combined.to_pandas()\n",
    "\n",
    "# Filter out non english entries \n",
    "prompt_injections_dataset_combined_df_en = prompt_injections_dataset_combined_df[prompt_injections_dataset_combined_df['text'].apply(lambda x: is_english(x))]\n",
    "\n",
    "prompt_injections_dataset_combined_df_en_sample = prompt_injections_dataset_combined_df_en.sample(frac=0.5, random_state=33).head(number_samples_from_each).dropna()\n",
    "\n",
    "prompt_injections_dataset_combined_df_en_sample['label'] = prompt_injections_dataset_combined_df_en_sample['label'].map({0: False, 1: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_injections_dataset_combined_df_en_sample = run_shield_evaluation(prompt_injections_dataset_combined_df_en_sample, prompt_injection_task, prompt_injection_rule)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "prompt_injections_dataset_combined_df_en_sample.to_csv(f\"./results/prompt_injections_dataset_combined_df_en_sample_{current_datetime}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(prompt_injections_dataset_combined_df_en_sample)\n",
    "\n",
    "custom_fn, custom_fp, custom_tp, custom_tn = granular_result_dfs(prompt_injections_dataset_combined_df_en_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
