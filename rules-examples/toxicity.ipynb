{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Rule Demonstration Notebook (SEE WARNING BELOW)\n",
    "\n",
    "**WARNING: THIS NOTEBOOK TESTS TOXIC MESSAGES WHICH MAY CONTAIN VERY OFFENSIVE LANGUAGE. IT IS ONLY FOR DEMONSTRATION PURPOSES**\n",
    "\n",
    "In this notebook we will walk through the following steps in order to demonstrate what the Shield Toxicity Rule is capable of. Toxicity prevents against: \n",
    "\n",
    "- Harmful or Illegal Requests\n",
    "- Profanity \n",
    "- Toxic tone  \n",
    "\n",
    "Pre  Requisites: \n",
    "- A Shield env and API key.\n",
    "\n",
    "1. Create a new task for Toxicity evaluation \n",
    "2. Arthur Benchmark dataset evaluation \n",
    "   1.  Run the examples against a pre-configured Shield task from Step 1 \n",
    "   2. Visualize our results - gain a better understanding of various result scores and determine best treshold to use \n",
    "3. Additional examples evaluation using datasets referenced in our documentation: https://shield.docs.arthur.ai/docs/toxicity#benchmarks\n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "   2. Visualize our results - gain a better understanding of various result scores and determine best treshold to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Configure Shield Test Env Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install scikit-learn\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, set_up_task_and_rule, run_shield_evaluation\n",
    "from analysis_utils import print_performance_metrics, granular_result_dfs, plot_toxicity_scores_distrubution\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Create a new task for Toxicity evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_rule_config=  {\n",
    "    \"name\": \"Toxicity Rule (0.75)\",\n",
    "    \"type\": \"ToxicityRule\",\n",
    "    \"apply_to_prompt\": True,\n",
    "    \"apply_to_response\": True,\n",
    "    \"config\": {\n",
    "        \"threshold\": 0.75\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Create task, archive all rules except the one we pass, create the rule we pass \n",
    "toxicity_rule, toxicity_task = set_up_task_and_rule(toxicity_rule_config, \"toxicity-task\")\n",
    "\n",
    "print(toxicity_rule)\n",
    "print(toxicity_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Arthur Benchmark dataset evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_combined_arthur_df = pd.read_csv(\"./arthur_benchmark_datasets/toxicitycombined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_combined_arthur_df = run_shield_evaluation(toxicity_combined_arthur_df, toxicity_task, toxicity_rule)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# Extract Shield score from full response \n",
    "def extract_toxicity_score(row): \n",
    "    return row.shield_result_full_detail[\"rule_results\"][0][\"details\"][\"toxicity_score\"]\n",
    "\n",
    "\n",
    "toxicity_combined_arthur_df[\"shield_score\"] = toxicity_combined_arthur_df.apply(extract_toxicity_score, axis=1).apply(pd.Series)\n",
    "toxicity_combined_arthur_df.to_csv(f\"./results/toxicity_combined_arthur_df_results_{current_datetime}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(toxicity_combined_arthur_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_toxicity_scores_distrubution(toxicity_combined_arthur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_combined_arthur_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic analysis of performance metrics using various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(row, test_threshold):\n",
    "    if row['shield_score'] > test_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    mixed_toxicity_df_dynamic = toxicity_combined_arthur_df.copy()\n",
    "    mixed_toxicity_df_dynamic[\"shield_result\"] = mixed_toxicity_df_dynamic.apply(evaluate_threshold, args=(threshold,), axis=1)\n",
    "    print(f\"----Test Threshold {threshold}----\")\n",
    "    print_performance_metrics(mixed_toxicity_df_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect results at various thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_threshold = 0.75\n",
    "\n",
    "toxicity_combined_arthur_df_dynamic = toxicity_combined_arthur_df.copy()\n",
    "\n",
    "toxicity_combined_arthur_df_dynamic[\"shield_result\"] = toxicity_combined_arthur_df_dynamic.apply(evaluate_threshold, args=(inspect_threshold,), axis=1)\n",
    "\n",
    "arthur_fn, arthur_fp, arthur_tp, arthur_tn = granular_result_dfs(toxicity_combined_arthur_df_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Load and sample benchmark datasets from https://shield.docs.arthur.ai/docs/toxicity#benchmarks\n",
    "\n",
    "**DISCLAIMER**: This is for demonstration and guidance purposes only and does not reflect the performance of the model behind the Shield score, as sampling techniques may not be optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_lmsys = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")\n",
    "toxicity_lmsys = pd.DataFrame(toxicity_lmsys[\"test\"])\n",
    "\n",
    "toxicity_wiki_toxic = load_dataset(\"OxAISH-AL-LLM/wiki_toxic\")\n",
    "toxicity_wiki_toxic = pd.DataFrame(toxicity_wiki_toxic[\"test\"])\n",
    "\n",
    "toxicity_harmfulrequest = pd.read_csv(\"./datasets/toxicity_harmfulrequest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows to pull from each toxicity type dataset. Will be evaluated against your Shield instance.\n",
    "number_samples_from_each = 50\n",
    "\n",
    "sample_lmsys = toxicity_lmsys.sample(frac=0.01, random_state=55).head(number_samples_from_each).dropna()\n",
    "sample_wiki_oxai = toxicity_wiki_toxic.sample(frac=0.01, random_state=55).head(number_samples_from_each).dropna()\n",
    "\n",
    "sample_harmfulrequest = toxicity_harmfulrequest.sample(frac=0.9, random_state=55).head(number_samples_from_each).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lmsys_df = sample_lmsys.loc[:, ['user_input', 'human_annotation']]\n",
    "sample_lmsys_df[\"source\"] = \"lmsys\"\n",
    "\n",
    "sample_wiki_oxai_df = sample_wiki_oxai.loc[:, ['comment_text', 'label']]\n",
    "\n",
    "sample_lmsys_df.rename(columns={'user_input': 'text', 'human_annotation': 'binary_label'}, inplace=True)\n",
    "sample_wiki_oxai_df.rename(columns={'comment_text': 'text'}, inplace=True)\n",
    "sample_wiki_oxai_df.rename(columns={'label': 'binary_label'}, inplace=True)\n",
    "sample_wiki_oxai_df['binary_label'] = sample_wiki_oxai_df['binary_label'].map({0: False, 1: True})\n",
    "sample_wiki_oxai_df[\"source\"] = \"wiki_oxai\"\n",
    "\n",
    "sample_harmfulrequest_df = sample_harmfulrequest.loc[:, ['text', 'binary_label', 'source']]\n",
    "\n",
    "mixed_toxicity_df = pd.concat([sample_lmsys_df, sample_wiki_oxai_df, sample_harmfulrequest_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_toxicity_df = run_shield_evaluation(mixed_toxicity_df, toxicity_task, toxicity_rule)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# Extract Shield score from full response \n",
    "def extract_toxicity_score(row): \n",
    "    return row.shield_result_full_detail[\"rule_results\"][0][\"details\"][\"toxicity_score\"]\n",
    "\n",
    "\n",
    "mixed_toxicity_df[\"shield_score\"] = mixed_toxicity_df.apply(extract_toxicity_score, axis=1).apply(pd.Series)\n",
    "mixed_toxicity_df.to_csv(f\"./results/mixed_toxicity_df_{current_datetime}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_toxicity_scores_distrubution(mixed_toxicity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic analysis of performance metrics using various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(row, test_threshold):\n",
    "    if row['shield_score'] > test_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    mixed_toxicity_df_dynamic = toxicity_combined_arthur_df.copy()\n",
    "    mixed_toxicity_df_dynamic[\"shield_result\"] = mixed_toxicity_df_dynamic.apply(evaluate_threshold, args=(threshold,), axis=1)\n",
    "    print(f\"----Test Threshold {threshold}----\")\n",
    "    print_performance_metrics(mixed_toxicity_df_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect results at various thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_threshold = 0.75\n",
    "\n",
    "toxicity_combined_arthur_df_dynamic = toxicity_combined_arthur_df.copy()\n",
    "\n",
    "toxicity_combined_arthur_df_dynamic[\"shield_result\"] = toxicity_combined_arthur_df_dynamic.apply(evaluate_threshold, args=(inspect_threshold,), axis=1)\n",
    "\n",
    "custom_fn, custom_fp, custom_tp, custom_tn = granular_result_dfs(toxicity_combined_arthur_df_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
