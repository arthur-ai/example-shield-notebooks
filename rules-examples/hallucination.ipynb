{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Rule Demonstration Notebook\n",
    "\n",
    "In this notebook we will walk through the following steps in order to demonstrate what the Shield Hallucination Rule is capable of.\n",
    "\n",
    "- Hallucination detected.\n",
    "- No hallucination detected. \n",
    "- Not evaluated for hallucinaion.\n",
    "\n",
    "Pre  Requisites: \n",
    "- A Shield env and API key.\n",
    "\n",
    "1. Create a new task for Hallucination evaluation \n",
    "2. Arthur Benchmark dataset evaluation \n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "   2. View our results \n",
    "3. Additional examples evaluation using datasets referenced in our documentation: https://shield.docs.arthur.ai/docs/hallucination#benchmarks\n",
    "   1. Run the examples against a pre-configured Shield task from Step 1 \n",
    "    2. View our results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Shield Test Env Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install scikit-learn\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from os.path import abspath, join\n",
    "import sys\n",
    "import random\n",
    "\n",
    "utils_path = abspath(join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from shield_utils import setup_env, set_up_task_and_rule, run_shield_evaluation\n",
    "from analysis_utils import print_performance_metrics, granular_result_dfs\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "setup_env(base_url=\"<URL>\", api_key=\"<API_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.Setup: Configure a test task and enable Prompt Injection Rule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_rule_config =  {\n",
    "    \"name\": \"Hallucination Rule\",\n",
    "    \"type\": \"ModelHallucinationRuleV2\",\n",
    "    \"apply_to_prompt\": False,\n",
    "    \"apply_to_response\": True\n",
    "}\n",
    "\n",
    "# Create task, archive all rules except the one we pass, create the rule we pass \n",
    "hallucination_rule, hallucination_task = set_up_task_and_rule(hallucination_rule_config, \"pii-task\")\n",
    "\n",
    "print(hallucination_rule)\n",
    "print(hallucination_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Arthur benchmark dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_benchmark_df_arthur = pd.read_csv(\"./arthur_benchmark_datasets/hallucination_benchmark_dummy.csv\")\n",
    "\n",
    "hallucination_benchmark_df_arthur['label'] = hallucination_benchmark_df_arthur['binary_label'].map({0: False, 1: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid task {'id': '276c85b0-55f1-48a1-bcb2-cb5d6d806df8', 'name': 'hallucination-test-task-2257', 'created_at': 1711732552332, 'updated_at': 1711732552332, 'rules': [{'id': '9f3737b0-395c-4420-81d1-b56851d983d4', 'name': 'Hallucination Rule', 'type': 'ModelHallucinationRuleV2', 'apply_to_prompt': False, 'apply_to_response': True, 'enabled': True, 'scope': 'task', 'created_at': 1711732552481, 'updated_at': 1711732552481, 'config': None}]}\n",
      "Fail\n",
      "Pass\n",
      "Fail\n"
     ]
    }
   ],
   "source": [
    "if (len(hallucination_task[\"rules\"]) > 1):\n",
    "    raise Exception(\"Cannot have more than one rule enabled for this test.\")\n",
    "else: \n",
    "    if hallucination_task[\"rules\"][0][\"type\"] != \"ModelHallucinationRuleV2\":\n",
    "            raise Exception(\"Invalid rule type enabled. Must be PromptInjectionRule.\")\n",
    "    else: \n",
    "         print(f\"Valid task {hallucination_task}\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "task_id = hallucination_task[\"id\"]\n",
    "\n",
    "def shield_hallucination_evaluation(row): \n",
    "    shield_prompt_inference = task_prompt_validation(\"dummy\", 1, task_id)\n",
    "\n",
    "    inference_id = shield_prompt_inference[\"inference_id\"]\n",
    "\n",
    "    shield_result = task_response_validation(row.text, row.context, inference_id, task_id)\n",
    "\n",
    "    # TODO - add the reason or claims breakdown \n",
    "    \n",
    "    for rule_result in shield_result[\"rule_results\"]:\n",
    "        if rule_result[\"id\"] == hallucination_rule[\"id\"]:\n",
    "            result = rule_result[\"result\"]\n",
    "\n",
    "            print(result)\n",
    "            if result == \"Pass\": \n",
    "                result = False\n",
    "            else:\n",
    "                result = True\n",
    "\n",
    "            return result\n",
    "        \n",
    "\n",
    "hallucination_benchmark_df_arthur[\"shield_result\"] = hallucination_benchmark_df_arthur.apply(shield_hallucination_evaluation, axis=1).apply(pd.Series)\n",
    "\n",
    "# # Save to CSV to avoid having to run this again to view results \n",
    "hallucination_benchmark_df_arthur.to_csv(f\"./results/hallucination_benchmark_df_arthur_{current_datetime}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(hallucination_benchmark_df_arthur)\n",
    "\n",
    "arthur_fn, arthur_fp, arthur_tp, arthur_tn = granular_result_dfs(hallucination_benchmark_df_arthur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Load benchmark datasets from https://shield.docs.arthur.ai/docs/hallucination#benchmarks\n",
    "\n",
    "**DISCLAIMER**: This is for demonstration and guidance purposes only and does not reflect the performance of the model behind the Shield score, as sampling techniques may not be optimal. \n",
    "**DISCLAIMER 2**: This dataset contains German and Arthur prompt injection is only trained on English. We do our best to filter it out, but some German may slip through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Run the examples against a pre-configured Shield task from Step 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Analyze Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
