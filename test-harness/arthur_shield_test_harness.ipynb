{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efac5cfd-12ec-4038-b9f7-1522ebd0e3e5",
   "metadata": {},
   "source": [
    "# Arthur Shield Test Harness\n",
    "\n",
    "The code below can be used to perform a test of the PII, Prompt Injection, Sensitivity, and Toxicity detection capabilities of Arthur Shield.\n",
    "\n",
    "### Pre-Requisites: \n",
    "- You will need to configure which checks you would like to perform within Arthur Shield. See https://shield.docs.arthur.ai/docs/rule-configuration-guide\n",
    "- For non-hallucination checks:\n",
    "  - You will need to compile a CSV file containing test prompts for each of the rules you would like to evaluate. You can reference the main template CSV in this folder for formatting, and use it to create your own list of prompts. The columns your input CSV should include are as follows (case-sensitive): \n",
    "    * *id* - A unique ID for the prompt\n",
    "    * *prompt* - The text you are sending to Shield\n",
    "    * *flag* - Which flag you expect this prompt to trigger. Must choose one of the following (ENUM): pii, prompt_injection, sensitivity, toxicity, control\n",
    "- For hallucination checks: \n",
    "  - You will need to compile a separate CSV file from the one above containing test prompts for hallucination rules.\n",
    "  - For each example, you will provide both a prompt (e.g. a question about an employee manual) and context (e.g. a paragraph from the employee manual). You should include two types of prompts:\n",
    "      - *Test Prompts*: Factual questions which can be directly answered by the information provided in the context\n",
    "      - *Control Prompts*: Factual questions requesting information that is unrelated to the information provided in the context (i.e. the context does not provide the answer to the question)\n",
    "  - You can reference the hallucination template CSV in this folder for formatting, and use it to create your own list of prompts. The columns your input CSV should include are as follows (case-sensitive):\n",
    "    * *id* - A unique ID for the prompt\n",
    "    * *prompt* - The prompt you are sending to the LLM (should be a question related to the context, or an unrelated control question)\n",
    "    * *context* - The context that the LLM should reference to answer the prompt question (or unrelated if control)\n",
    "    * *context_answers_prompt* - Whether the answer to the question can be found in the prompt context or not. Bool value (\"True\" or \"False\")\n",
    "- Please aim to have at least 15 (and preferably more) prompts for each of the rules you would like to test in order to gather accurate analysis results, including controls\n",
    "\n",
    "### Output:\n",
    "This harness will output 2 CSVs: \n",
    "- Test output file containing info on the prompt, Shield flags, and latency\n",
    "- Metrics file containing various evaluation performance metrics for each of the Shield rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f346b-0779-4a53-ae23-6d9c6f22cbee",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Fill out the config below with the details specific to your Shield instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a34cf7-fc8c-4209-8d8f-f932509bc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests \n",
    "from datetime import datetime\n",
    "\n",
    "SHIELD_VAL_ENDPOINT = \"<your-shield-endpoint>/api/v2/validate_prompt\"\n",
    "SHIELD_API_KEY = \"<insert-shield-api-key>\"\n",
    "\n",
    "MAIN_INPUT_FILE = 'shield_main_input.csv' # Change this to whatever your input file name is\n",
    "MAIN_OUTPUT_FILE = 'shield_main_output.csv'\n",
    "METRICS_FILE = 'shield_test_metrics.csv'\n",
    "\n",
    "shield_headers = {\n",
    "    'Authorization': f'Bearer {SHIELD_API_KEY}'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421941e3-78c0-49d7-85f6-23dbc82da6f7",
   "metadata": {},
   "source": [
    "## i. Non-Hallucination Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd710ae-df76-4532-ab9d-ad294ffbe902",
   "metadata": {},
   "source": [
    "### Support Functions\n",
    "\n",
    "Shield's response schema can be found on the API docs: https://shield.jpmc-poc.arthur.ai/docs#/Default%20Validation/default_validate_prompt_api_v2_validate_prompt_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789476f-c80d-497e-b72f-6b1906adda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firewall_flags(resp_dict):\n",
    "    # Determines which rules (PII, Prompt Injection, Toxicity, Sensitivity) were flagged\n",
    "    flags = []\n",
    "    for rule in resp_dict[\"rule_results\"]:\n",
    "        if rule[\"result\"] == \"Fail\":\n",
    "            if rule[\"rule_type\"] == \"ModelHallucinationRuleV2\":\n",
    "                # Only used for responses - does not check for other flags\n",
    "                flags.append(\"hallucination\")\n",
    "                break\n",
    "            elif rule[\"rule_type\"] == \"PIIDataRule\" and \"pii\" not in flags:\n",
    "                flags.append(\"pii\")\n",
    "            elif rule[\"rule_type\"] == \"PromptInjectionRule\" and \"prompt_injection\" not in flags:\n",
    "                flags.append(\"prompt_injection\")\n",
    "            elif rule[\"rule_type\"] == \"ToxicityRule\" and \"toxicity\" not in flags:\n",
    "                flags.append(\"toxicity\")\n",
    "            elif rule[\"rule_type\"] == \"ModelSensitiveDataRule\" and \"sensitivity\" not in flags:\n",
    "                flags.append(\"sensitivity\")\n",
    "    return flags\n",
    "\n",
    "def get_gt_output_match(row):\n",
    "    # Returns True if GT label matches Shield flags and False otherwise\n",
    "    if row[\"ground_truth\"] == \"control\": \n",
    "        return False if row[\"shield_response\"] else True\n",
    "    else:\n",
    "        return True if row[\"ground_truth\"] in row[\"shield_response\"] else False\n",
    "    \n",
    "\n",
    "def get_pii_triggers(shield_response):\n",
    "    # Isolate the text which triggered the PII rule to fire\n",
    "    output = []\n",
    "    for rule in shield_response[\"rule_results\"]:\n",
    "        if rule[\"rule_type\"]== \"PIIDataRule\" and rule[\"result\"]==\"Fail\":\n",
    "            for flag in rule[\"details\"][\"pii_entities\"]:\n",
    "                output.append({\n",
    "                    \"string_trigger\": flag[\"span\"],\n",
    "                    \"flag_type\": flag[\"entity\"]\n",
    "                })\n",
    "    return output\n",
    "\n",
    "def get_shield_response(row):\n",
    "    # Calls Shield API and updates the row dictionary based upon the results from Shield\n",
    "    shield_start = datetime.now()\n",
    "    response = requests.post(SHIELD_VAL_ENDPOINT, headers=shield_headers, json={\"prompt\": row[\"prompt\"]})\n",
    "    shield_end = datetime.now()\n",
    "    row[\"latency_ms\"] = int((shield_end - shield_start).microseconds/1000)\n",
    "    row[\"shield_response\"] = get_firewall_flags(response.json())\n",
    "    row[\"gt_output_match\"] = get_gt_output_match(row)\n",
    "    row[\"sub_flags\"] = get_pii_triggers(response.json()) if \"pii\" in row[\"shield_response\"] else None\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54eb91f-eb2e-4c52-a744-7d017c57285b",
   "metadata": {},
   "source": [
    "### Generate Output File\n",
    "\n",
    "The output file generated will contain: \n",
    "  * *id* - The ID of the corresponding input file prompt\n",
    "  * *prompt* - The prompt that was passed\n",
    "  * *ground_truth* - The flag that was expected\n",
    "  * *shield_response* - The flag that Shield raised\n",
    "  * *test_passed* - Whether the ground truth label matches the flag that Shield raised (True or False)\n",
    "  * *sub-flags* - (For PII) Which sentences were flagged and which PII flag was raised\n",
    "  * *latency_ms* - Latency of Shield call in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84df830-dd5c-4b22-a774-7f8255acc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MAIN_INPUT_FILE, newline='') as infile:\n",
    "    reader= csv.DictReader(infile)\n",
    "    with open(MAIN_OUTPUT_FILE, 'w', newline='') as outfile:\n",
    "        # List of columns for the output file\n",
    "        fieldnames = [\n",
    "            \"id\",\n",
    "            \"prompt\",\n",
    "            \"ground_truth\",\n",
    "            \"shield_response\",\n",
    "            \"gt_output_match\",\n",
    "            \"sub_flags\",\n",
    "            \"latency_ms\"\n",
    "        ]\n",
    "        writer= csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in reader:\n",
    "            out_row = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"ground_truth\": row[\"flag\"],\n",
    "                \"prompt\": row[\"prompt\"]\n",
    "                }\n",
    "\n",
    "            out_row = get_shield_response(out_row)\n",
    "            writer.writerow(out_row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906f76e-2136-470d-bcf0-b27c700b4dd9",
   "metadata": {},
   "source": [
    "## ii. Hallucination Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d582a19-d93b-4cce-8323-f23ee0f9266a",
   "metadata": {},
   "source": [
    "In order to perform hallucination checks, you will need to connect to an LLM. Below is a simple API connection; if your LLM requires a different setup, you may have to modify this block to fit your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665a268-efc2-4fd9-a3e8-57215b0b90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIELD_RESP_ENDPOINT = \"<your-shield-endpoint>/api/v2/validate_response/\"\n",
    "\n",
    "LLM_ENDPOINT = \"<llm-endpoint>\"\n",
    "LLM_API_KEY = \"<llm-api-key>\"\n",
    "\n",
    "HALLUCINATION_INPUT = 'shield_hallucination_input_template.csv' # Change this to whatever your input file name is \n",
    "HALLUCINATION_OUTPUT = 'shield_hallucination_output.csv'\n",
    "\n",
    "#Note - you may need to customize how the LLM connection works per your setup\n",
    "llm_headers = {\n",
    "    'Authorization': f'Bearer {LLM_API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1876a7-1304-4cf9-aa2e-0ed1693e49d9",
   "metadata": {},
   "source": [
    "### Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1400d-16ab-4c11-9490-e91b6bb73654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claims(response):\n",
    "    # Extracts the claims from the Shield flag \n",
    "    for rule in response[\"rule_results\"]:\n",
    "        if rule[\"rule_type\"] != \"ModelHallucinationRuleV2\":\n",
    "            continue\n",
    "        else:\n",
    "            if rule[\"result\"] == \"Pass\":\n",
    "                return []\n",
    "            else:\n",
    "                return rule[\"details\"][\"claims\"]\n",
    "\n",
    "def get_inference_id(prompt):\n",
    "    # Conduct an initial Shield call to get an inference ID for the response endpoint\n",
    "    response = (requests.post(\n",
    "        SHIELD_VAL_ENDPOINT, \n",
    "        headers=shield_headers, \n",
    "        json={\"prompt\": prompt})).json()\n",
    "    return str(response[\"inference_id\"])\n",
    "\n",
    "## NOTE: May need to re-write the below function based upon your LLM setup. This function is \n",
    "## compatible with OpenAI's API\n",
    "def get_llm_response(prompt, context):\n",
    "    # Get the LLM response to the initial prompt, passing along the context provided\n",
    "    # Note: This data JSON format is specific to OpenAI; may need modification for other \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(LLM_ENDPOINT, headers=llm_headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")    \n",
    "\n",
    "        \n",
    "def get_hallucination_shield_response(row):\n",
    "    # Calls Shield API and updates the row dictionary based upon the results from Shield\n",
    "    resp_endpoint = SHIELD_RESP_ENDPOINT + get_inference_id(row[\"prompt\"])\n",
    "    row[\"llm_response\"] = get_llm_response(row[\"prompt\"], row[\"context\"])\n",
    "    shield_start = datetime.now()\n",
    "    response = requests.post(\n",
    "        resp_endpoint, \n",
    "        headers=shield_headers, \n",
    "        json={\"response\": row[\"llm_response\"], \"context\": row[\"context\"]}\n",
    "    )\n",
    "    shield_end = datetime.now()\n",
    "    row[\"latency_ms\"] = int((shield_end - shield_start).microseconds/1000)\n",
    "    row[\"shield_response\"] = get_firewall_flags(response.json())\n",
    "    row[\"valid_response\"] = \"\"\n",
    "    row[\"claims\"] = get_claims(response.json())\n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8147d-f600-4287-b0b4-0c48dd62f716",
   "metadata": {},
   "source": [
    "### Generate Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab0ca3-d1bf-426a-a94e-3ecdafeb62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HALLUCINATION_INPUT, newline='') as infile:\n",
    "    reader= csv.DictReader(infile)\n",
    "    with open(HALLUCINATION_OUTPUT, 'w', newline='') as outfile:\n",
    "        # List of columns for the output file\n",
    "        fieldnames = [\n",
    "            \"id\",\n",
    "            \"prompt\",\n",
    "            \"context\",\n",
    "            \"context_answers_prompt\",\n",
    "            \"llm_response\",\n",
    "            \"shield_response\",\n",
    "            \"valid_response\",\n",
    "            \"claims\",\n",
    "            \"latency_ms\"\n",
    "        ]\n",
    "        writer= csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in reader:\n",
    "            out_row = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"context_answers_prompt\": row[\"context_answers_prompt\"],\n",
    "                \"prompt\": row[\"prompt\"],\n",
    "                \"context\": row[\"context\"]\n",
    "                }\n",
    "\n",
    "            out_row = get_hallucination_shield_response(out_row)\n",
    "            writer.writerow(out_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf1d9d-d7fc-4373-be13-0698707344cf",
   "metadata": {},
   "source": [
    "**NOTE: In order to run analysis on the hallucination output, you will need to manually classify each output example as valid or invalid.**\n",
    "\n",
    "Under the \"valid_response\" column of the output file, you will assess each statement's results to determine whether or not the Shield response is valid. You will enter \"True\" or \"False\" (case sensitive). Here's a guide for classifying the responses:\n",
    "- valid_response = True if:\n",
    "    - The \"shield_response\" flagged the statement as a hallucination, and the \"llm_response\" did indeed hallucinate when answering the question (i.e. LLM responded with information not included in the context).\n",
    "    - The \"shield_response\" did not flag the statement as a hallucination, and the \"llm_response\" either a) did not attempt to answer the question, or b) correctly answered the question with information it retrieved from the context\n",
    "- valid_response = False if:\n",
    "    - The \"shield_response\" flagged the statement as a hallucination, but the \"llm_response\" either a) correctly responded to the question using the provided context or b) did not answer the question and did not provide any information not included in the context\n",
    "    - The \"shield_response\" did not flag the statement as a hallucination, but the \"llm_response\" attempted to answer the question with information that was not included in the provided context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7593c-b3ac-4d3b-b529-a728dfa79bc5",
   "metadata": {},
   "source": [
    "## iii. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da0e84-18c5-4516-97e4-e60bb60c511b",
   "metadata": {},
   "source": [
    "The metrics file generated will contain:\n",
    "  * True Positive Count\n",
    "  * False Positive Count\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * Specificity\n",
    "  * Miss Rate\n",
    "  * False Positive Rate\n",
    "  * F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbdd0f-9a68-459f-9fe8-b2a00f215757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_row(data_row, precision=3):\n",
    "    return [round(item, precision) if isinstance(item, (float, int)) else item for item in data_row]\n",
    "\n",
    "def create_metrics_dict(metric_name):\n",
    "    if metric_name == \"hallucination\":\n",
    "        hdf= pd.read_csv(HALLUCINATION_OUTPUT)\n",
    "        metrics = {\n",
    "            \"tp\": hdf[(hdf[\"valid_response\"]==True) & (hdf['shield_response'].apply(lambda x: metric_name in x))].shape[0],\n",
    "            \"fp\": hdf[(hdf[\"valid_response\"]==False) & (hdf['shield_response'].apply(lambda x: metric_name in x))].shape[0],\n",
    "            \"fn\": hdf[(hdf[\"valid_response\"]==False) & (hdf['shield_response'].apply(lambda x: metric_name not in x))].shape[0],\n",
    "            \"tn\": hdf[(hdf[\"valid_response\"]==True) & (hdf['shield_response'].apply(lambda x: metric_name not in x))].shape[0]\n",
    "        }\n",
    "    else:\n",
    "        df = pd.read_csv(MAIN_OUTPUT_FILE)\n",
    "        metrics = {\n",
    "            \"tp\": df[(df['gt_output_match']==True) & (df['ground_truth']==metric_name)].shape[0],\n",
    "            \"fp\": df[(df['ground_truth']!=metric_name) & (df['shield_response'].apply(lambda x: metric_name in x))].shape[0],\n",
    "            \"fn\": df[(df['ground_truth']==metric_name) & (df['gt_output_match']==False)].shape[0],\n",
    "            \"tn\": df[(df['ground_truth']!=metric_name) & (df['shield_response'].apply(lambda x: metric_name not in x))].shape[0]\n",
    "        }\n",
    "    try:\n",
    "        metrics[\"prec\"] = metrics[\"tp\"]/(metrics[\"tp\"]+metrics[\"fp\"])\n",
    "    except: \n",
    "        metrics[\"prec\"] = \"N/A\"\n",
    "    try:\n",
    "        metrics[\"recall\"] = metrics[\"tp\"]/(metrics[\"tp\"]+metrics[\"fn\"])\n",
    "    except:\n",
    "        metrics[\"recall\"] = \"N/A\"\n",
    "    return metrics\n",
    "\n",
    "def run_analysis(checks=[\"pii\", \"prompt_injection\", \"toxicity\", \"sensitivity\", \"hallucination\"]):\n",
    "    # Runs analysis on the selected checks. Runs on all by default\n",
    "    with open(METRICS_FILE, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        metrics = []\n",
    "        for check in checks:\n",
    "            metrics.append(create_metrics_dict(check))\n",
    "        writer.writerow([\"\"]+checks)\n",
    "    \n",
    "        # True Positive Count\n",
    "        tpc= [\"True Positive Count\"]\n",
    "        for metric in metrics:\n",
    "            tpc.append(metric[\"tp\"])\n",
    "        writer.writerow(round_row(tpc))\n",
    "    \n",
    "        # False Positive Count\n",
    "        fpc= [\"False Positive Count\"]\n",
    "        for metric in metrics:\n",
    "            fpc.append(metric[\"fp\"])\n",
    "        writer.writerow(round_row(fpc))\n",
    "                        \n",
    "        # Precision\n",
    "        prec = [\"Precision\"]\n",
    "        for metric in metrics:\n",
    "            prec.append(metric[\"prec\"])\n",
    "        writer.writerow(round_row(prec))\n",
    "        \n",
    "        # Recall\n",
    "        recall = [\"Recall\"]\n",
    "        for metric in metrics:\n",
    "            recall.append(metric[\"recall\"])\n",
    "        writer.writerow(round_row(recall))\n",
    "\n",
    "        # Specificity\n",
    "        spec = [\"Specificity\"]\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                spec.append(metric[\"tn\"]/(metric[\"tn\"]+metric[\"fp\"]))\n",
    "            except:\n",
    "                spec.append(\"N/A\")\n",
    "        writer.writerow(round_row(spec))\n",
    "    \n",
    "        # Miss Rate\n",
    "        miss = [\"Miss Rate\"]\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                miss.append(metric[\"fn\"]/(metric[\"fn\"]+metric[\"tp\"]))\n",
    "            except:\n",
    "                miss.append(\"N/A\")\n",
    "        writer.writerow(round_row(miss))\n",
    "        \n",
    "        # False Positive Rate\n",
    "        fpr = [\"False Positive Rate\"]\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                fpr.append(metric[\"fp\"]/(metric[\"fp\"]+metric[\"tn\"]))\n",
    "            except:\n",
    "                fpr.append(\"N/A\")\n",
    "        writer.writerow(round_row(fpr))\n",
    "    \n",
    "        # F1 Score\n",
    "        f1 = [\"F1 Score\"]\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                f1.append((2*metric[\"prec\"]*metric[\"recall\"])/(metric[\"prec\"]+metric[\"recall\"]))\n",
    "            except:\n",
    "                f1.append(\"N/A\")\n",
    "        writer.writerow(round_row(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46655a4-04bd-4b0a-a069-fb0882df7967",
   "metadata": {},
   "source": [
    "The cell below will run analysis on all of the flags included in the list (the list can contain \"pii\", \"prompt_injection\", \"toxicity\", and/or \"sensitivity\"). **If any of these flags are not enabled in the Shield instance you are evaluating, omit it from the input list to avoid generating errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcbf8a-bb59-4b8e-b22f-d241fe06689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_analysis([\n",
    "    \"pii\",\n",
    "    \"prompt_injection\",\n",
    "    \"toxicity\",\n",
    "    \"sensitivity\",\n",
    "    \"hallucination\"\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
